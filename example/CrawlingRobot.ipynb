{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UCb7z8qthxj7",
    "outputId": "9989a973-536c-44c7-b5ed-1f808b3d20bf"
   },
   "outputs": [],
   "source": [
    "#!git clone git@github.com:micheltokic/crawlingrobot.git\n",
    "#!pip install -e crawlingrobot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pygame\n",
    "import os\n",
    "#os.environ['SDL_VIDEODRIVER']='dummy'\n",
    "import gymnasium as gym\n",
    "import gym_crawlingrobot\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_robot_control (env):\n",
    "    \n",
    "    done = False\n",
    "    action = None\n",
    "    obs, _ = env.reset()\n",
    "    print(f\"initial state: {obs}\")\n",
    "    cum_reward = 0\n",
    "    step = 0\n",
    "\n",
    "    while True:\n",
    "        # process pygame event loop\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_ESCAPE:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "                elif event.key == pygame.K_UP or event.key == pygame.K_w:\n",
    "                    env.step(0)\n",
    "                    action = 0\n",
    "                elif event.key == pygame.K_RIGHT or event.key == pygame.K_d:\n",
    "                    action = 1\n",
    "                elif event.key == pygame.K_DOWN or event.key == pygame.K_s:\n",
    "                    action = 2\n",
    "                elif event.key == pygame.K_LEFT or event.key == pygame.K_a:\n",
    "                    action = 3\n",
    "                elif event.key == pygame.K_r:\n",
    "                    env.reset()\n",
    "                    action = 3\n",
    "                elif event.key == pygame.K_SPACE:\n",
    "                    env.robot.render_intermediate_steps = not env.robot.render_intermediate_steps\n",
    "\n",
    "                if action:\n",
    "                    obs, reward, terminated, truncated, info = env.step(action)\n",
    "                    done = terminated or truncated\n",
    "                    cum_reward += reward\n",
    "                    print (f\"step={step}, obs={obs}, action={action}, reward={reward:.2f}, cum_reward={cum_reward:.2f}, done={done}\")\n",
    "\n",
    "                    action = None \n",
    "                    step += 1\n",
    "                if done:\n",
    "                    env.reset()\n",
    "                    action = 3\n",
    "                    cum_reward = 0\n",
    "                    step = 0\n",
    "\n",
    "            env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\backup\\git\\crawlingrobot\\.venv\\Lib\\site-packages\\gymnasium\\envs\\registration.py:481: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes']\u001b[0m\n",
      "  logger.warn(\n",
      "D:\\backup\\git\\crawlingrobot\\.venv\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.robot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.robot` for environment variables or `env.get_wrapper_attr('robot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "D:\\backup\\git\\crawlingrobot\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be int64, actual type: int32\u001b[0m\n",
      "  logger.warn(\n",
      "D:\\backup\\git\\crawlingrobot\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:318: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state: [0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\backup\\git\\crawlingrobot\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be int64, actual type: int32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0, obs=[1 0], action=2, reward=0.99, cum_reward=0.99, done=False\n",
      "step=1, obs=[2 0], action=2, reward=0.01, cum_reward=1.00, done=False\n",
      "step=2, obs=[3 0], action=2, reward=7.24, cum_reward=8.24, done=False\n",
      "step=3, obs=[4 0], action=2, reward=13.38, cum_reward=21.63, done=False\n",
      "step=4, obs=[4 1], action=3, reward=22.67, cum_reward=44.29, done=False\n",
      "step=5, obs=[4 2], action=3, reward=23.59, cum_reward=67.89, done=False\n",
      "step=6, obs=[2 1], action=1, reward=-0.01, cum_reward=67.88, done=False\n",
      "step=7, obs=[2 0], action=1, reward=0.01, cum_reward=67.89, done=False\n",
      "step=8, obs=[2 0], action=1, reward=0.00, cum_reward=67.89, done=False\n",
      "step=9, obs=[3 0], action=2, reward=6.79, cum_reward=74.69, done=False\n",
      "step=10, obs=[4 0], action=2, reward=13.69, cum_reward=88.38, done=False\n",
      "step=11, obs=[4 1], action=3, reward=22.72, cum_reward=111.09, done=False\n",
      "step=12, obs=[4 2], action=3, reward=23.22, cum_reward=134.31, done=False\n"
     ]
    }
   ],
   "source": [
    "pygame.quit() # close any already opened simulation windows\n",
    "\n",
    "env = gym.make('crawlingrobot-discrete-v1', rotation_angles=5, goal_distance=700, window_size=(640, 480), render_intermediate_steps=True, plot_steps_per_episode=True)\n",
    "env.robot.mode = 2 # => Use WASD or Arrow Keys to control the robot's arms\n",
    "manual_robot_control (env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Q-Learning with discrete actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "0uw6R_AY2-zH",
    "outputId": "ddeb06e2-713e-45a4-9925-a5368bdce981"
   },
   "outputs": [],
   "source": [
    "# function maps the 2D observation (x, y) to a single state number n \n",
    "def obs_to_number(obs, obs_max):\n",
    "    return int(obs[0] * obs_max + obs[1])\n",
    "\n",
    "def q_agent(Q, obs_max, env, learn=True, render=False, alpha=1, gamma=0.95, epsilon=0.2, maxSteps=10000, episodes=200):\n",
    "    \n",
    "    print (f\"Q.shape={Q.shape}\")\n",
    "    np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "    for episode in range (episodes):\n",
    "        done = False\n",
    "        init_obs, _ = env.reset()\n",
    "        init_obs = init_obs.tolist()\n",
    "        state = obs_to_number(init_obs, obs_max)\n",
    "        step = 0\n",
    "        cum_reward =0 \n",
    "\n",
    "        while not done and step < maxSteps:\n",
    "\n",
    "            # action selection\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "\n",
    "            # perform action in environment\n",
    "            nextObs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            nextObs = nextObs.tolist()\n",
    "            nextState = obs_to_number(nextObs, obs_max)\n",
    "            cum_reward += reward\n",
    "\n",
    "            # environment rendering\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "                # process pygame event loop\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                        return\n",
    "                    elif event.type == pygame.KEYDOWN:\n",
    "                        if event.key == pygame.K_ESCAPE:\n",
    "                            pygame.quit()\n",
    "                            return\n",
    "                        if event.key == pygame.K_SPACE:\n",
    "                            env.robot.render_intermediate_steps = not env.robot.render_intermediate_steps\n",
    "\n",
    "            # Q-learning\n",
    "            if learn:\n",
    "                Q[state, action] += alpha * (reward + gamma * np.max(Q[nextState]) - Q[state, action])\n",
    "\n",
    "            # time transition\n",
    "            state = nextState\n",
    "            step += 1\n",
    "            \n",
    "        res = 0\n",
    "        if len(env.robot.episode_time_results) > 0:\n",
    "            res = env.robot.episode_time_results[-1]\n",
    "        print(f\"episode={episode} took {step} steps => cumulative reward: {cum_reward:.2f}\")\n",
    "        \n",
    "    pygame.quit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Learn Q function (no GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q.shape=(25, 4)\n",
      "episode=0 took 560 steps => cumulative reward: 512.63\n",
      "episode=1 took 168 steps => cumulative reward: 515.29\n",
      "episode=2 took 97 steps => cumulative reward: 529.99\n",
      "episode=3 took 116 steps => cumulative reward: 514.23\n",
      "episode=4 took 96 steps => cumulative reward: 512.84\n",
      "episode=5 took 99 steps => cumulative reward: 518.24\n",
      "episode=6 took 107 steps => cumulative reward: 529.97\n",
      "episode=7 took 103 steps => cumulative reward: 517.51\n",
      "episode=8 took 105 steps => cumulative reward: 530.90\n",
      "episode=9 took 92 steps => cumulative reward: 527.40\n",
      "Wrote Q function to file:  Qfunction.pkl\n"
     ]
    }
   ],
   "source": [
    "pygame.quit() # close any already opened simulation windows\n",
    "\n",
    "# instantiate environment\n",
    "env = gym.make('crawlingrobot-discrete-v1', rotation_angles=5, goal_distance=700)\n",
    "\n",
    "# 2.1) Initialize Q function\n",
    "obs_max = env.observation_space.high[0] + 1  # currently 5\n",
    "Q = np.zeros([obs_max ** len(env.observation_space.high), env.action_space.n])\n",
    "q_filename = \"Qfunction.pkl\"\n",
    "\n",
    "# learn Q function\n",
    "q_agent(Q=Q, obs_max=obs_max, env=env, gamma=0.9, epsilon=0.1, episodes=10, render=False, learn=True)\n",
    "\n",
    "# write learned Q function to disc\n",
    "pickle.dump( Q, open( q_filename, \"wb\" ) )\n",
    "print (\"Wrote Q function to file: \", q_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Evaluate policy derived from Q function (with GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Q function from file:  Qfunction.pkl\n",
      "Q.shape=(25, 4)\n",
      "episode=0 took 114 steps => cumulative reward: 521.63\n",
      "episode=1 took 112 steps => cumulative reward: 525.30\n"
     ]
    }
   ],
   "source": [
    "pygame.quit() # close any already opened simulation windows\n",
    "\n",
    "# load Q function\n",
    "print (\"Loading Q function from file: \", q_filename)\n",
    "Q = pickle.load( open(q_filename, \"rb\" ) )\n",
    "\n",
    "# evalue Q function\n",
    "env = gym.make('crawlingrobot-discrete-v1', rotation_angles=5, goal_distance=700, window_size=(640, 480), plot_steps_per_episode=True)\n",
    "q_agent(Q=Q, obs_max=obs_max, env=env, episodes=20, epsilon=0.1, render=True, learn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpxQT37sIuRK"
   },
   "source": [
    "# 3) PPO control with continuous actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rfj_Nq08JEJg",
    "outputId": "d0fbe109-ba32-45ab-b8e6-cb0ad51b3d7b"
   },
   "outputs": [],
   "source": [
    "import pygame\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import PPO\n",
    "import gymnasium as gym\n",
    "import gym_crawlingrobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callback class for event loop cleanup\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class PyGameEventLoopCallback(BaseCallback):\n",
    "    \n",
    "    render = False\n",
    "    training_env = None\n",
    "    \n",
    "    def __init__(self, verbose=0, render=False):\n",
    "        super(PyGameEventLoopCallback, self).__init__(verbose)\n",
    "        self.render = render\n",
    "        # Those variables will be accessible in the callback\n",
    "        # (they are defined in the base class)\n",
    "        # The RL model\n",
    "        # self.model = None  # type: BaseRLModel\n",
    "        # An alias for self.model.get_env(), the environment used for training\n",
    "        # self.training_env = None  # type: Union[gym.Env, VecEnv, None]\n",
    "        # Number of time the callback was called\n",
    "        # self.n_calls = 0  # type: int\n",
    "        # self.num_timesteps = 0  # type: int\n",
    "        # local and global variables\n",
    "        # self.locals = None  # type: Dict[str, Any]\n",
    "        # self.globals = None  # type: Dict[str, Any]\n",
    "        # The logger object, used to report things in the terminal\n",
    "        # self.logger = None  # type: logger.Logger\n",
    "        # # Sometimes, for event callback, it is useful\n",
    "        # # to have access to the parent object\n",
    "        # self.parent = None  # type: Optional[BaseCallback]\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        \n",
    "        robot_env = self.training_env.venv.envs[0]\n",
    "        \n",
    "        # process pygame event loop\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return False\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_ESCAPE:\n",
    "                    pygame.quit()\n",
    "                    return False\n",
    "                if event.key == pygame.K_SPACE:\n",
    "                    robot_env.robot.render_intermediate_steps = not robot_env.robot.render_intermediate_steps\n",
    "\n",
    "        if self.render:\n",
    "            robot_env.render()\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "25QXzNWkJTEs",
    "outputId": "4385d29c-55aa-46e3-f6f8-966bfe82d6c4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = \"\"\n",
    "ppo = \"ppo\"\n",
    "os.makedirs(ppo, exist_ok=True)\n",
    "\n",
    "def ppo_learn(env, render=False, total_timesteps=20000):\n",
    "    env = VecNormalize(DummyVecEnv([lambda: Monitor(env, log_dir)]), norm_obs=True, norm_reward=True)\n",
    "    model = PPO(env=env, policy=\"MlpPolicy\", verbose=1)\n",
    "\n",
    "    cb = PyGameEventLoopCallback(render=render)\n",
    "    cb.training_env = env\n",
    "\n",
    "    model.learn(total_timesteps=total_timesteps, callback=cb)\n",
    "    model.save(\"ppo/ppo_crawling_robot\")\n",
    "    env.save(\"ppo/vec_normalize.pkl\")\n",
    " \n",
    "    del model, env\n",
    "\n",
    "\n",
    "def ppo_run_policy(env, render=False, episodes=1, deterministic=True):\n",
    "    env = DummyVecEnv([lambda: Monitor(env, log_dir)])\n",
    "    env = VecNormalize.load(\"ppo/vec_normalize.pkl\", env)\n",
    "    env.training = False\n",
    "\n",
    "    model = PPO.load(\"ppo/ppo_crawling_robot\")\n",
    "\n",
    "    # visualization callback\n",
    "    cb = PyGameEventLoopCallback(render=render)\n",
    "    cb.training_env = env\n",
    "    \n",
    "    for e in range (episodes): \n",
    "\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        cum_reward = 0\n",
    "        step = 0\n",
    "\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs, deterministic=deterministic)\n",
    "            #obs, _reward, terminated, truncated, info = env.step(action)\n",
    "            #done = truncated or terminated\n",
    "            obs, _reward, done, info = env.step(action)\n",
    "            reward = env.get_original_reward() # returns the last unnormalized reward\n",
    "            if not cb._on_step(): \n",
    "                return\n",
    "            \n",
    "            cum_reward += reward[0]\n",
    "            step += 1\n",
    "            print (f\"episode={e}, step={step}, action={action}, reward={reward[0]:.2f}, cum_reward={cum_reward:.2f}, done={done}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train policy for 20000 timesteps (no GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\backup\\git\\crawlingrobot\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "D:\\backup\\git\\crawlingrobot\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "D:\\backup\\git\\crawlingrobot\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "D:\\backup\\git\\crawlingrobot\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 421  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.04e+03    |\n",
      "|    ep_rew_mean          | 2.38e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 382         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008621333 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | -0.379      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0608      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    std                  | 0.991       |\n",
      "|    value_loss           | 0.328       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.45e+03    |\n",
      "|    ep_rew_mean          | 2.35e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 350         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010503528 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | 0.01        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0509      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    std                  | 0.977       |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.34e+03    |\n",
      "|    ep_rew_mean          | 2.35e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 342         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011320766 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.78       |\n",
      "|    explained_variance   | 0.0261      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0404      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    std                  | 0.965       |\n",
      "|    value_loss           | 0.0879      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.02e+03    |\n",
      "|    ep_rew_mean          | 2.35e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 343         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012161362 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.0186      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0291      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    std                  | 0.947       |\n",
      "|    value_loss           | 0.0977      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 803         |\n",
      "|    ep_rew_mean          | 2.34e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 328         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014331444 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | 0.0289      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00102     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    std                  | 0.934       |\n",
      "|    value_loss           | 0.0639      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 640         |\n",
      "|    ep_rew_mean          | 2.34e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 330         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013447087 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.68       |\n",
      "|    explained_variance   | 0.00752     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0325      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    std                  | 0.92        |\n",
      "|    value_loss           | 0.086       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 528          |\n",
      "|    ep_rew_mean          | 2.34e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 331          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 49           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0124319885 |\n",
      "|    clip_fraction        | 0.166        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.66        |\n",
      "|    explained_variance   | -0.00941     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0855       |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0153      |\n",
      "|    std                  | 0.913        |\n",
      "|    value_loss           | 0.0993       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 457         |\n",
      "|    ep_rew_mean          | 2.33e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 333         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008849176 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.65       |\n",
      "|    explained_variance   | -0.00971    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0474      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    std                  | 0.902       |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 401          |\n",
      "|    ep_rew_mean          | 2.33e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 333          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069523165 |\n",
      "|    clip_fraction        | 0.0673       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.62        |\n",
      "|    explained_variance   | 0.0122       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0393       |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0045      |\n",
      "|    std                  | 0.897        |\n",
      "|    value_loss           | 0.158        |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pygame.quit() # close any already opened simulation windows\n",
    "\n",
    "robot_env = gym.make('crawlingrobot-continuous-v1', goal_distance=2500)\n",
    "ppo_learn(env=robot_env, render=False, total_timesteps=20000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate policy (with GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\backup\\git\\crawlingrobot\\.venv\\Lib\\site-packages\\gymnasium\\envs\\registration.py:481: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes']\u001b[0m\n",
      "  logger.warn(\n",
      "D:\\backup\\git\\crawlingrobot\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:318: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode=0, step=1, action=[[ 0.9224521  -0.80447966]], reward=18.49, cum_reward=18.49, done=[False]\n",
      "episode=0, step=2, action=[[0.90136594 0.9827521 ]], reward=63.71, cum_reward=82.20, done=[False]\n",
      "episode=0, step=3, action=[[-0.8699516  0.5694554]], reward=0.00, cum_reward=82.20, done=[False]\n",
      "episode=0, step=4, action=[[-0.0223683 -1.       ]], reward=0.00, cum_reward=82.20, done=[False]\n",
      "episode=0, step=5, action=[[ 0.94116026 -0.76198816]], reward=18.61, cum_reward=100.81, done=[False]\n",
      "episode=0, step=6, action=[[0.8892091 1.       ]], reward=62.75, cum_reward=163.57, done=[False]\n",
      "episode=0, step=7, action=[[-0.9040554  0.5372033]], reward=0.01, cum_reward=163.57, done=[False]\n",
      "episode=0, step=8, action=[[ 0.03220048 -1.        ]], reward=-0.00, cum_reward=163.57, done=[False]\n",
      "episode=0, step=9, action=[[ 0.95794904 -0.723708  ]], reward=19.38, cum_reward=182.95, done=[False]\n",
      "episode=0, step=10, action=[[0.8820782 1.       ]], reward=60.47, cum_reward=243.42, done=[False]\n",
      "episode=0, step=11, action=[[-0.9103266  0.5151225]], reward=0.01, cum_reward=243.43, done=[False]\n",
      "episode=0, step=12, action=[[ 0.07617337 -1.        ]], reward=-0.01, cum_reward=243.42, done=[False]\n"
     ]
    }
   ],
   "source": [
    "pygame.quit() # close any already opened simulation windows\n",
    "\n",
    "#robot_env_nogui = gym.make('crawlingrobot-continuous-v1', goal_distance=2500, plot_steps_per_episode=False, render_intermediate_steps=False)\n",
    "robot_env_gui = gym.make('crawlingrobot-continuous-v1', goal_distance=700, window_size=(640, 480), plot_steps_per_episode=True, render_intermediate_steps=True)\n",
    "ppo_run_policy(env=robot_env_gui, episodes=1, render=True, deterministic=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CrawlingRobot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
