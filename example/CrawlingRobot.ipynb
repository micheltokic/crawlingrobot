{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UCb7z8qthxj7",
    "outputId": "9989a973-536c-44c7-b5ed-1f808b3d20bf"
   },
   "outputs": [],
   "source": [
    "#!git clone git@github.com:micheltokic/crawlingrobot.git\n",
    "#!pip install -e crawlingrobot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.12.1)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pygame\n",
    "import os\n",
    "#os.environ['SDL_VIDEODRIVER']='dummy'\n",
    "import gymnasium as gym\n",
    "import gym_crawlingrobot\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_robot_control (env):\n",
    "    \n",
    "    done = False\n",
    "    action = None\n",
    "    obs, _ = env.reset()\n",
    "    print(f\"initial state: {obs}\")\n",
    "    cum_reward = 0\n",
    "    step = 0\n",
    "\n",
    "    while True:\n",
    "        # process pygame event loop\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_ESCAPE:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "                elif event.key == pygame.K_UP or event.key == pygame.K_w:\n",
    "                    env.step(0)\n",
    "                    action = 0\n",
    "                elif event.key == pygame.K_RIGHT or event.key == pygame.K_d:\n",
    "                    action = 1\n",
    "                elif event.key == pygame.K_DOWN or event.key == pygame.K_s:\n",
    "                    action = 2\n",
    "                elif event.key == pygame.K_LEFT or event.key == pygame.K_a:\n",
    "                    action = 3\n",
    "                elif event.key == pygame.K_r:\n",
    "                    env.reset()\n",
    "                    action = 3\n",
    "                elif event.key == pygame.K_SPACE:\n",
    "                    env.robot.render_intermediate_steps = not env.robot.render_intermediate_steps\n",
    "\n",
    "                if action:\n",
    "                    obs, reward, terminated, truncated, info = env.step(action)\n",
    "                    done = terminated or truncated\n",
    "                    cum_reward += reward\n",
    "                    print (f\"step={step}, obs={obs}, action={action}, reward={reward:.2f}, cum_reward={cum_reward:.2f}, done={done}\")\n",
    "\n",
    "                    action = None \n",
    "                    step += 1\n",
    "                if done:\n",
    "                    env.reset()\n",
    "                    action = 3\n",
    "                    cum_reward = 0\n",
    "                    step = 0\n",
    "\n",
    "            env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state: [0 0]\n",
      "step=0, obs=[1 0], action=2, reward=0.99, cum_reward=0.99, done=False\n",
      "step=1, obs=[2 0], action=2, reward=0.01, cum_reward=1.00, done=False\n",
      "step=2, obs=[3 0], action=2, reward=7.24, cum_reward=8.24, done=False\n",
      "step=3, obs=[4 0], action=2, reward=13.38, cum_reward=21.63, done=False\n",
      "step=4, obs=[4 1], action=3, reward=22.67, cum_reward=44.29, done=False\n",
      "step=5, obs=[4 2], action=3, reward=23.59, cum_reward=67.89, done=False\n",
      "step=6, obs=[4 3], action=3, reward=18.93, cum_reward=86.81, done=False\n",
      "step=7, obs=[1 2], action=1, reward=-0.01, cum_reward=86.80, done=False\n",
      "step=8, obs=[1 1], action=1, reward=0.02, cum_reward=86.82, done=False\n",
      "step=9, obs=[1 0], action=1, reward=-0.00, cum_reward=86.82, done=False\n",
      "step=10, obs=[1 0], action=1, reward=0.00, cum_reward=86.82, done=False\n",
      "step=11, obs=[2 0], action=2, reward=0.01, cum_reward=86.83, done=False\n",
      "step=12, obs=[3 0], action=2, reward=6.80, cum_reward=93.62, done=False\n",
      "step=13, obs=[4 0], action=2, reward=17.11, cum_reward=110.74, done=False\n",
      "step=14, obs=[4 0], action=2, reward=0.00, cum_reward=110.74, done=False\n",
      "step=15, obs=[4 0], action=2, reward=0.00, cum_reward=110.74, done=False\n",
      "step=16, obs=[4 1], action=3, reward=21.82, cum_reward=132.56, done=False\n",
      "step=17, obs=[4 2], action=3, reward=23.56, cum_reward=156.13, done=False\n",
      "step=18, obs=[4 3], action=3, reward=18.23, cum_reward=174.36, done=False\n",
      "step=19, obs=[4 4], action=3, reward=7.14, cum_reward=181.50, done=False\n",
      "step=20, obs=[0 3], action=1, reward=-0.02, cum_reward=181.47, done=False\n",
      "step=21, obs=[0 2], action=1, reward=0.02, cum_reward=181.49, done=False\n",
      "step=22, obs=[0 1], action=1, reward=-0.01, cum_reward=181.49, done=False\n",
      "step=23, obs=[0 0], action=1, reward=0.01, cum_reward=181.50, done=False\n",
      "step=24, obs=[0 0], action=1, reward=0.00, cum_reward=181.50, done=False\n",
      "step=25, obs=[0 0], action=1, reward=0.00, cum_reward=181.50, done=False\n",
      "step=26, obs=[0 0], action=1, reward=0.00, cum_reward=181.50, done=False\n",
      "step=27, obs=[0 0], action=1, reward=0.00, cum_reward=181.50, done=False\n",
      "step=28, obs=[0 0], action=1, reward=0.00, cum_reward=181.50, done=False\n",
      "step=29, obs=[0 0], action=1, reward=0.00, cum_reward=181.50, done=False\n",
      "step=30, obs=[0 0], action=1, reward=0.00, cum_reward=181.50, done=False\n",
      "step=31, obs=[0 0], action=1, reward=0.00, cum_reward=181.50, done=False\n",
      "step=32, obs=[1 0], action=2, reward=0.01, cum_reward=181.51, done=False\n",
      "step=33, obs=[2 0], action=2, reward=0.00, cum_reward=181.51, done=False\n",
      "step=34, obs=[3 0], action=2, reward=7.24, cum_reward=188.76, done=False\n",
      "step=35, obs=[4 0], action=2, reward=13.39, cum_reward=202.14, done=False\n",
      "step=36, obs=[4 0], action=2, reward=0.00, cum_reward=202.14, done=False\n",
      "step=37, obs=[4 0], action=2, reward=0.00, cum_reward=202.14, done=False\n",
      "step=38, obs=[4 0], action=2, reward=0.00, cum_reward=202.14, done=False\n",
      "step=39, obs=[4 0], action=2, reward=0.00, cum_reward=202.14, done=False\n",
      "step=40, obs=[4 1], action=3, reward=22.67, cum_reward=224.81, done=False\n",
      "step=41, obs=[4 2], action=3, reward=24.24, cum_reward=249.06, done=False\n",
      "step=42, obs=[4 3], action=3, reward=18.24, cum_reward=267.30, done=False\n",
      "step=43, obs=[0 2], action=1, reward=-0.01, cum_reward=267.29, done=False\n",
      "step=44, obs=[0 1], action=1, reward=0.01, cum_reward=267.30, done=False\n",
      "step=45, obs=[0 0], action=1, reward=-0.03, cum_reward=267.28, done=False\n",
      "step=46, obs=[0 0], action=1, reward=0.00, cum_reward=267.28, done=False\n",
      "step=47, obs=[0 0], action=1, reward=0.00, cum_reward=267.28, done=False\n",
      "step=48, obs=[0 0], action=1, reward=0.00, cum_reward=267.28, done=False\n",
      "step=49, obs=[0 0], action=1, reward=0.00, cum_reward=267.28, done=False\n",
      "step=50, obs=[1 0], action=2, reward=0.02, cum_reward=267.30, done=False\n",
      "step=51, obs=[2 0], action=2, reward=0.01, cum_reward=267.31, done=False\n",
      "step=52, obs=[3 0], action=2, reward=6.81, cum_reward=274.12, done=False\n",
      "step=53, obs=[4 0], action=2, reward=14.25, cum_reward=288.37, done=False\n",
      "step=54, obs=[4 0], action=2, reward=0.00, cum_reward=288.37, done=False\n",
      "step=55, obs=[4 0], action=2, reward=0.00, cum_reward=288.37, done=False\n",
      "step=56, obs=[4 1], action=3, reward=22.76, cum_reward=311.14, done=False\n",
      "step=57, obs=[4 2], action=3, reward=23.03, cum_reward=334.17, done=False\n",
      "step=58, obs=[4 3], action=3, reward=18.70, cum_reward=352.87, done=False\n",
      "step=59, obs=[0 2], action=1, reward=-0.01, cum_reward=352.86, done=False\n",
      "step=60, obs=[0 1], action=1, reward=0.01, cum_reward=352.87, done=False\n",
      "step=61, obs=[0 0], action=1, reward=-0.03, cum_reward=352.85, done=False\n",
      "step=62, obs=[0 0], action=1, reward=0.00, cum_reward=352.85, done=False\n",
      "step=63, obs=[0 0], action=1, reward=0.00, cum_reward=352.85, done=False\n",
      "step=64, obs=[0 0], action=1, reward=0.00, cum_reward=352.85, done=False\n",
      "step=65, obs=[0 0], action=1, reward=0.00, cum_reward=352.85, done=False\n",
      "step=66, obs=[0 0], action=1, reward=0.00, cum_reward=352.85, done=False\n",
      "step=67, obs=[1 0], action=2, reward=0.02, cum_reward=352.87, done=False\n",
      "step=68, obs=[2 0], action=2, reward=0.01, cum_reward=352.88, done=False\n",
      "step=69, obs=[3 0], action=2, reward=7.24, cum_reward=360.12, done=False\n",
      "step=70, obs=[4 0], action=2, reward=13.38, cum_reward=373.51, done=False\n",
      "step=71, obs=[4 0], action=2, reward=0.00, cum_reward=373.51, done=False\n",
      "step=72, obs=[4 1], action=3, reward=22.66, cum_reward=396.17, done=False\n",
      "step=73, obs=[4 2], action=3, reward=24.60, cum_reward=420.78, done=False\n",
      "step=74, obs=[4 3], action=3, reward=18.46, cum_reward=439.23, done=False\n"
     ]
    }
   ],
   "source": [
    "pygame.quit() # close any already opened simulation windows\n",
    "\n",
    "env = gym.make('crawlingrobot-discrete-v1', rotation_angles=5, goal_distance=700, window_size=(640, 480), render_intermediate_steps=True, plot_steps_per_episode=True)\n",
    "env.robot.mode = 2 # => Use WASD or Arrow Keys to control the robot's arms\n",
    "manual_robot_control (env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Q-Learning with discrete actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "0uw6R_AY2-zH",
    "outputId": "ddeb06e2-713e-45a4-9925-a5368bdce981"
   },
   "outputs": [],
   "source": [
    "# function maps the 2D observation (x, y) to a single state number n \n",
    "def obs_to_number(obs, obs_max):\n",
    "    return int(obs[0] * obs_max + obs[1])\n",
    "\n",
    "def q_agent(Q, obs_max, env, learn=True, render=False, alpha=1, gamma=0.95, epsilon=0.2, maxSteps=10000, episodes=200):\n",
    "    \n",
    "    print (f\"Q.shape={Q.shape}\")\n",
    "    np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "    for episode in range (episodes):\n",
    "        done = False\n",
    "        init_obs, _ = env.reset()\n",
    "        init_obs = init_obs.tolist()\n",
    "        state = obs_to_number(init_obs, obs_max)\n",
    "        step = 0\n",
    "        cum_reward =0 \n",
    "\n",
    "        while not done and step < maxSteps:\n",
    "\n",
    "            # action selection\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "\n",
    "            # perform action in environment\n",
    "            nextObs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            nextObs = nextObs.tolist()\n",
    "            nextState = obs_to_number(nextObs, obs_max)\n",
    "            cum_reward += reward\n",
    "\n",
    "            # environment rendering\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "                # process pygame event loop\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                        return\n",
    "                    elif event.type == pygame.KEYDOWN:\n",
    "                        if event.key == pygame.K_ESCAPE:\n",
    "                            pygame.quit()\n",
    "                            return\n",
    "                        if event.key == pygame.K_SPACE:\n",
    "                            env.robot.render_intermediate_steps = not env.robot.render_intermediate_steps\n",
    "\n",
    "            # Q-learning\n",
    "            if learn:\n",
    "                Q[state, action] += alpha * (reward + gamma * np.max(Q[nextState]) - Q[state, action])\n",
    "\n",
    "            # time transition\n",
    "            state = nextState\n",
    "            step += 1\n",
    "            \n",
    "        res = 0\n",
    "        if len(env.robot.episode_time_results) > 0:\n",
    "            res = env.robot.episode_time_results[-1]\n",
    "        print(f\"episode={episode} took {step} steps => cumulative reward: {cum_reward:.2f}\")\n",
    "        \n",
    "    pygame.quit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Learn Q function (no GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q.shape=(25, 4)\n",
      "episode=0 took 822 steps => cumulative reward: 513.65\n",
      "episode=1 took 170 steps => cumulative reward: 511.26\n",
      "episode=2 took 86 steps => cumulative reward: 531.51\n",
      "episode=3 took 105 steps => cumulative reward: 530.28\n",
      "episode=4 took 103 steps => cumulative reward: 524.10\n",
      "episode=5 took 86 steps => cumulative reward: 530.22\n",
      "episode=6 took 68 steps => cumulative reward: 510.14\n",
      "episode=7 took 74 steps => cumulative reward: 527.97\n",
      "episode=8 took 79 steps => cumulative reward: 515.19\n",
      "episode=9 took 88 steps => cumulative reward: 519.94\n",
      "Wrote Q function to file:  Qfunction.pkl\n"
     ]
    }
   ],
   "source": [
    "pygame.quit() # close any already opened simulation windows\n",
    "\n",
    "# instantiate environment\n",
    "env = gym.make('crawlingrobot-discrete-v1', rotation_angles=5, goal_distance=700)\n",
    "\n",
    "# 2.1) Initialize Q function\n",
    "obs_max = env.observation_space.high[0] + 1  # currently 5\n",
    "Q = np.zeros([obs_max ** len(env.observation_space.high), env.action_space.n])\n",
    "q_filename = \"Qfunction.pkl\"\n",
    "\n",
    "# learn Q function\n",
    "q_agent(Q=Q, obs_max=obs_max, env=env, gamma=0.9, epsilon=0.1, episodes=10, render=False, learn=True)\n",
    "\n",
    "# write learned Q function to disc\n",
    "pickle.dump( Q, open( q_filename, \"wb\" ) )\n",
    "print (\"Wrote Q function to file: \", q_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Evaluate policy derived from Q function (with GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Q function from file:  Qfunction.pkl\n",
      "Q.shape=(25, 4)\n",
      "episode=0 took 72 steps => cumulative reward: 521.52\n",
      "episode=1 took 84 steps => cumulative reward: 514.85\n",
      "episode=2 took 74 steps => cumulative reward: 515.84\n",
      "episode=3 took 78 steps => cumulative reward: 518.03\n",
      "episode=4 took 77 steps => cumulative reward: 516.10\n",
      "episode=5 took 79 steps => cumulative reward: 514.02\n",
      "episode=6 took 79 steps => cumulative reward: 512.45\n",
      "episode=7 took 72 steps => cumulative reward: 518.08\n",
      "episode=8 took 77 steps => cumulative reward: 511.87\n",
      "episode=9 took 81 steps => cumulative reward: 520.30\n"
     ]
    }
   ],
   "source": [
    "pygame.quit() # close any already opened simulation windows\n",
    "\n",
    "# load Q function\n",
    "print (\"Loading Q function from file: \", q_filename)\n",
    "Q = pickle.load( open(q_filename, \"rb\" ) )\n",
    "\n",
    "# evalue Q function\n",
    "env = gym.make('crawlingrobot-discrete-v1', rotation_angles=5, goal_distance=700, window_size=(640, 480), plot_steps_per_episode=True)\n",
    "q_agent(Q=Q, obs_max=obs_max, env=env, episodes=20, epsilon=0.1, render=True, learn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpxQT37sIuRK"
   },
   "source": [
    "# 3) PPO control with continuous actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rfj_Nq08JEJg",
    "outputId": "d0fbe109-ba32-45ab-b8e6-cb0ad51b3d7b"
   },
   "outputs": [],
   "source": [
    "import pygame\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import PPO\n",
    "import gymnasium as gym\n",
    "import gym_crawlingrobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callback class for event loop cleanup\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class PyGameEventLoopCallback(BaseCallback):\n",
    "    \n",
    "    render = False\n",
    "    training_env = None\n",
    "    \n",
    "    def __init__(self, verbose=0, render=False):\n",
    "        super(PyGameEventLoopCallback, self).__init__(verbose)\n",
    "        self.render = render\n",
    "        # Those variables will be accessible in the callback\n",
    "        # (they are defined in the base class)\n",
    "        # The RL model\n",
    "        # self.model = None  # type: BaseRLModel\n",
    "        # An alias for self.model.get_env(), the environment used for training\n",
    "        # self.training_env = None  # type: Union[gym.Env, VecEnv, None]\n",
    "        # Number of time the callback was called\n",
    "        # self.n_calls = 0  # type: int\n",
    "        # self.num_timesteps = 0  # type: int\n",
    "        # local and global variables\n",
    "        # self.locals = None  # type: Dict[str, Any]\n",
    "        # self.globals = None  # type: Dict[str, Any]\n",
    "        # The logger object, used to report things in the terminal\n",
    "        # self.logger = None  # type: logger.Logger\n",
    "        # # Sometimes, for event callback, it is useful\n",
    "        # # to have access to the parent object\n",
    "        # self.parent = None  # type: Optional[BaseCallback]\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        \n",
    "        robot_env = self.training_env.venv.envs[0]\n",
    "        \n",
    "        # process pygame event loop\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_ESCAPE:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "                if event.key == pygame.K_SPACE:\n",
    "                    robot_env.robot.render_intermediate_steps = not robot_env.robot.render_intermediate_steps\n",
    "\n",
    "        if self.render:\n",
    "            robot_env.render()\n",
    "        \n",
    "        return bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "25QXzNWkJTEs",
    "outputId": "4385d29c-55aa-46e3-f6f8-966bfe82d6c4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = \"\"\n",
    "ppo = \"ppo\"\n",
    "os.makedirs(ppo, exist_ok=True)\n",
    "\n",
    "def ppo_learn(env, render=False):\n",
    "    env = VecNormalize(DummyVecEnv([lambda: Monitor(env, log_dir)]), norm_obs=True, norm_reward=True)\n",
    "    model = PPO(env=env, policy=\"MlpPolicy\", verbose=1)\n",
    "\n",
    "    cb = PyGameEventLoopCallback(render=render)\n",
    "    cb.training_env = env\n",
    "\n",
    "    model.learn(total_timesteps=10000, callback=cb)\n",
    "    model.save(\"ppo/ppo_crawling_robot\")\n",
    "    env.save(\"ppo/vec_normalize.pkl\")\n",
    " \n",
    "    del model, env\n",
    "\n",
    "\n",
    "def ppo_run_policy(env, render=False, episodes=1):\n",
    "    env = DummyVecEnv([lambda: Monitor(env, log_dir)])\n",
    "    env = VecNormalize.load(\"ppo/vec_normalize.pkl\", env)\n",
    "    env.training = False\n",
    "\n",
    "    model = PPO.load(\"ppo/ppo_crawling_robot\")\n",
    "\n",
    "    # visualization callback\n",
    "    cb = PyGameEventLoopCallback(render=render)\n",
    "    cb.training_env = env\n",
    "    \n",
    "    for e in range (episodes): \n",
    "\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        cum_reward = 0\n",
    "        step = 0\n",
    "\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs)\n",
    "            #obs, _reward, terminated, truncated, info = env.step(action)\n",
    "            #done = truncated or terminated\n",
    "            obs, _reward, done, info = env.step(action)\n",
    "            reward = env.get_original_reward() # returns the last unnormalized reward\n",
    "            cb._on_step()\n",
    "            cum_reward += reward[0]\n",
    "            step += 1\n",
    "            print (f\"episode={e}, step={step}, action={action}, reward={reward[0]:.2f}, cum_reward={cum_reward:.2f}, done={done}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train policy for 30000 timesteps (no GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z003c4dc\\crawlingrobot\\.venv\\Lib\\site-packages\\gymnasium\\envs\\registration.py:481: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z003c4dc\\crawlingrobot\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\z003c4dc\\crawlingrobot\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "C:\\Users\\z003c4dc\\crawlingrobot\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\z003c4dc\\crawlingrobot\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 591  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.52e+03    |\n",
      "|    ep_rew_mean          | 2.32e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 485         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010187434 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.185      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.168       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.365       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.84e+03    |\n",
      "|    ep_rew_mean          | 2.33e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 455         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012299774 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.0221      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0537      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    std                  | 0.979       |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.32e+03    |\n",
      "|    ep_rew_mean          | 2.34e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 446         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012601195 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.78       |\n",
      "|    explained_variance   | 0.0316      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00183     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    std                  | 0.965       |\n",
      "|    value_loss           | 0.0892      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.01e+03    |\n",
      "|    ep_rew_mean          | 2.34e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 440         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012723148 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | -0.00962    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0392      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    std                  | 0.952       |\n",
      "|    value_loss           | 0.102       |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pygame.quit() # close any already opened simulation windows\n",
    "\n",
    "robot_env = gym.make('crawlingrobot-continuous-v1', goal_distance=2500)\n",
    "ppo_learn(env=robot_env, render=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate policy (with GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z003c4dc\\crawlingrobot\\.venv\\Lib\\site-packages\\gymnasium\\envs\\registration.py:481: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes']\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\z003c4dc\\crawlingrobot\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:318: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode=0, step=1, action=[[1.        0.6310337]], reward=17.23, cum_reward=17.23, done=[False]\n",
      "episode=0, step=2, action=[[0.33713335 1.        ]], reward=2.31, cum_reward=19.54, done=[False]\n",
      "episode=0, step=3, action=[[-0.40605515  0.263824  ]], reward=0.01, cum_reward=19.55, done=[False]\n",
      "episode=0, step=4, action=[[ 0.42536035 -0.7386334 ]], reward=0.01, cum_reward=19.56, done=[False]\n",
      "episode=0, step=5, action=[[1.        0.3145724]], reward=39.92, cum_reward=59.48, done=[False]\n",
      "episode=0, step=6, action=[[ 1.        -0.7551183]], reward=-38.07, cum_reward=21.42, done=[False]\n",
      "episode=0, step=7, action=[[-0.10277799  0.05253655]], reward=-15.85, cum_reward=5.57, done=[False]\n",
      "episode=0, step=8, action=[[ 0.30469382 -1.        ]], reward=-0.01, cum_reward=5.56, done=[False]\n",
      "episode=0, step=9, action=[[-0.44443786  0.09460967]], reward=0.02, cum_reward=5.58, done=[False]\n",
      "episode=0, step=10, action=[[ 0.6909639  -0.65997636]], reward=4.95, cum_reward=10.52, done=[False]\n",
      "episode=0, step=11, action=[[0.90718365 0.5307046 ]], reward=61.83, cum_reward=72.35, done=[False]\n",
      "episode=0, step=12, action=[[-1.         0.8283937]], reward=3.67, cum_reward=76.02, done=[False]\n",
      "episode=0, step=13, action=[[ 0.2950527 -1.       ]], reward=0.02, cum_reward=76.04, done=[False]\n",
      "episode=0, step=14, action=[[-1.         -0.01396537]], reward=-0.02, cum_reward=76.02, done=[False]\n",
      "episode=0, step=15, action=[[0.32625455 0.67231375]], reward=-0.01, cum_reward=76.01, done=[False]\n",
      "episode=0, step=16, action=[[-1.          0.17026742]], reward=-0.00, cum_reward=76.01, done=[False]\n",
      "episode=0, step=17, action=[[ 0.44534332 -1.        ]], reward=0.01, cum_reward=76.02, done=[False]\n",
      "episode=0, step=18, action=[[0.8149401 0.5480141]], reward=50.07, cum_reward=126.09, done=[False]\n",
      "episode=0, step=19, action=[[-0.3890118   0.34468085]], reward=-41.92, cum_reward=84.17, done=[False]\n",
      "episode=0, step=20, action=[[ 0.48148495 -0.43560076]], reward=-0.00, cum_reward=84.17, done=[False]\n",
      "episode=0, step=21, action=[[-0.6230156   0.31306237]], reward=-0.00, cum_reward=84.17, done=[False]\n",
      "episode=0, step=22, action=[[ 0.38416657 -0.2476922 ]], reward=0.02, cum_reward=84.19, done=[False]\n",
      "episode=0, step=23, action=[[ 1. -1.]], reward=22.27, cum_reward=106.46, done=[False]\n",
      "episode=0, step=24, action=[[-0.30361834 -0.63490605]], reward=-26.82, cum_reward=79.64, done=[False]\n",
      "episode=0, step=25, action=[[-1. -1.]], reward=-0.11, cum_reward=79.53, done=[False]\n",
      "episode=0, step=26, action=[[-0.75313026 -1.        ]], reward=0.38, cum_reward=79.91, done=[False]\n",
      "episode=0, step=27, action=[[0.53893375 0.16121632]], reward=-0.19, cum_reward=79.72, done=[False]\n",
      "episode=0, step=28, action=[[0.67751825 1.        ]], reward=41.58, cum_reward=121.30, done=[False]\n",
      "episode=0, step=29, action=[[ 0.25282174 -0.64150864]], reward=0.11, cum_reward=121.41, done=[False]\n",
      "episode=0, step=30, action=[[-0.14862573 -0.17062189]], reward=-0.00, cum_reward=121.41, done=[False]\n",
      "episode=0, step=31, action=[[ 1.         -0.08622384]], reward=21.79, cum_reward=143.20, done=[False]\n",
      "episode=0, step=32, action=[[0.6783194  0.28814566]], reward=10.28, cum_reward=153.48, done=[False]\n",
      "episode=0, step=33, action=[[ 1.        -0.3025643]], reward=-11.59, cum_reward=141.88, done=[False]\n",
      "episode=0, step=34, action=[[-1.  1.]], reward=12.51, cum_reward=154.39, done=[False]\n",
      "episode=0, step=35, action=[[-0.29631853  0.96337384]], reward=0.07, cum_reward=154.46, done=[False]\n",
      "episode=0, step=36, action=[[-1.          0.04623979]], reward=-0.06, cum_reward=154.40, done=[False]\n",
      "episode=0, step=37, action=[[-0.64982265 -0.36762497]], reward=0.05, cum_reward=154.45, done=[False]\n",
      "episode=0, step=38, action=[[0.797963   0.76604337]], reward=-0.05, cum_reward=154.40, done=[False]\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#robot_env_nogui = gym.make('crawlingrobot-continuous-v1', goal_distance=2500, plot_steps_per_episode=False, render_intermediate_steps=False)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m robot_env_gui \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrawlingrobot-continuous-v1\u001b[39m\u001b[38;5;124m'\u001b[39m, goal_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m700\u001b[39m, window_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m480\u001b[39m), plot_steps_per_episode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, render_intermediate_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mppo_run_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrobot_env_gui\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 41\u001b[0m, in \u001b[0;36mppo_run_policy\u001b[1;34m(env, render, episodes)\u001b[0m\n\u001b[0;32m     38\u001b[0m action, _states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#obs, _reward, terminated, truncated, info = env.step(action)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#done = truncated or terminated\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m obs, _reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m reward \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_original_reward() \u001b[38;5;66;03m# returns the last unnormalized reward\u001b[39;00m\n\u001b[0;32m     43\u001b[0m cb\u001b[38;5;241m.\u001b[39m_on_step()\n",
      "File \u001b[1;32m~\\crawlingrobot\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\crawlingrobot\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_normalize.py:181\u001b[0m, in \u001b[0;36mVecNormalize.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m    175\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m    Apply sequence of actions to sequence of environments\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m    actions -> (observations, rewards, dones)\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    where ``dones`` is a boolean vector indicating whether each element is new.\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m     obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, (np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m))  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_obs \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[1;32m~\\crawlingrobot\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[1;32m~\\crawlingrobot\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[1;32m~\\crawlingrobot\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\crawlingrobot\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\crawlingrobot\\gym_crawlingrobot\\envs\\crawlingrobot_continuous_env.py:20\u001b[0m, in \u001b[0;36mCrawlingRobotContinuousEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mcontains(action)\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrobot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrobot\u001b[38;5;241m.\u001b[39mget_observation()\n\u001b[0;32m     22\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrobot\u001b[38;5;241m.\u001b[39mget_distance()\n",
      "File \u001b[1;32m~\\crawlingrobot\\gym_crawlingrobot\\envs\\crawlingRobot.py:230\u001b[0m, in \u001b[0;36mCrawlingRobot.action\u001b[1;34m(self, target_angles)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_intermediate_steps \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_render_initialized):\n\u001b[1;32m--> 230\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_render_initialized:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_camera()\n",
      "File \u001b[1;32m~\\crawlingrobot\\gym_crawlingrobot\\envs\\crawlingRobot.py:310\u001b[0m, in \u001b[0;36mCrawlingRobot.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_render_initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_render()\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    312\u001b[0m font \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mfont\u001b[38;5;241m.\u001b[39mSysFont(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVerdana\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m90\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    313\u001b[0m text1, text2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "pygame.quit() # close any already opened simulation windows\n",
    "\n",
    "#robot_env_nogui = gym.make('crawlingrobot-continuous-v1', goal_distance=2500, plot_steps_per_episode=False, render_intermediate_steps=False)\n",
    "robot_env_gui = gym.make('crawlingrobot-continuous-v1', goal_distance=700, window_size=(640, 480), plot_steps_per_episode=True, render_intermediate_steps=True)\n",
    "ppo_run_policy(env=robot_env_gui, episodes=30, render=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CrawlingRobot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
